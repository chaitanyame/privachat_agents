# PrivaChat Agents

Privacy-focused AI agent system with advanced search, research, and document processing capabilities. Built with Pydantic AI agents, RAG, and multi-modal LLM support.

---

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Quick Start](#-quick-start-3-steps)
- [Architecture](#-architecture)
- [Technology Stack](#-technology-stack)
- [Usage](#-usage)
- [Configuration](#-configuration)
- [Development](#-development)
- [Testing](#-testing)
- [API Reference](#-api-reference)
- [Deployment](#-deployment)
- [Project Status](#-project-status)

---

## âœ¨ Features

### Core Capabilities

- **ğŸ” Fast Search Mode** (30-60s): Multi-query decomposition with intelligent source aggregation
- **ğŸ“š Deep Research Mode** (2-5 min): Multi-step iterative research with comprehensive reports
- **ğŸ“„ Document Library**: Upload and query PDF, Word, Excel files with RAG-powered Q&A
- **ğŸ¤– Pydantic AI Agents**: Structured multi-agent system (SearchAgent, ResearchAgent, DocumentAgent)
- **ğŸ§  RAG System**: PostgreSQL + pgvector for semantic search with 384D embeddings
- **ğŸ“Š Document Processing**: Extract and process PDF, Excel, Word files with Dockling
- **ğŸŒ Web Crawling**: Deep website extraction with Crawl4AI
- **ğŸ“ˆ LLM Monitoring**: Complete tracing with Langfuse integration

### Advanced Features

- **Citation Grounding**: Automatic hallucination detection with claim verification
- **Multi-Source Synthesis**: Intelligent triangulation, conflict resolution, chronological ordering
- **Content-Type Adaptation**: Academic, news, technical content-aware responses
- **Configurable Prompts**: Static, dynamic, or auto prompt strategies
- **Multi-Search Engine**: SearxNG (primary), SerperDev (fallback), Perplexity (optional)
- **Semantic Reranking**: Enhanced relevance scoring with embedding-based reranking
- **Perplexity Integration**: Circuit-breaker protected fallback search

---

## ğŸš€ Quick Start (3 Steps)

### Prerequisites

- **Docker Desktop** (Windows/Mac) or **Docker Engine + Docker Compose** (Linux)
- **API Keys**:
  - [OpenRouter API Key](https://openrouter.ai/keys) - For LLM access
  - [Langfuse Keys](https://cloud.langfuse.com) - For monitoring (recommended)
  - [SerperDev API Key](https://serper.dev/) - For search (optional if using SearxNG)

### Step 1: Setup Environment

```bash
cd research-service

# Copy environment template
cp .env.example .env

# Edit .env with your API keys
nano .env  # or use any text editor
```

**Required in `.env`:**
```bash
OPENROUTER_API_KEY=your_openrouter_key_here
LANGFUSE_PUBLIC_KEY=your_langfuse_public_key
LANGFUSE_SECRET_KEY=your_langfuse_secret_key
SERPER_API_KEY=your_serper_key_here  # Optional
```

### Step 2: Start Services

**Using Scripts:**
```bash
# Linux/Mac
./quickstart.sh

# Windows
quickstart.bat
```

**Or Docker Compose:**
```bash
docker-compose up -d
```

### Step 3: Access the UI

- **ğŸ¨ Streamlit UI**: http://localhost:8501 â† Start here!
- **ğŸ“– API Docs**: http://localhost:8001/api/docs
- **ğŸ’š Health Check**: http://localhost:8001/api/v1/health

---

## ğŸ—ï¸ Architecture

```
research-service/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ api/v1/                  # FastAPI endpoints
â”‚   â”‚   â”œâ”€â”€ endpoints/           # Route handlers
â”‚   â”‚   â”‚   â”œâ”€â”€ search.py       # Search endpoint
â”‚   â”‚   â”‚   â”œâ”€â”€ research.py     # Research endpoint
â”‚   â”‚   â”‚   â””â”€â”€ documents.py    # Document upload/query
â”‚   â”‚   â””â”€â”€ schemas.py           # Pydantic models
â”‚   â”œâ”€â”€ agents/                  # Pydantic AI agents
â”‚   â”‚   â”œâ”€â”€ search_agent.py     # Multi-query search
â”‚   â”‚   â”œâ”€â”€ research_agent.py   # Multi-step research
â”‚   â”‚   â””â”€â”€ prompt_strategy.py  # Configurable prompts
â”‚   â”œâ”€â”€ services/                # External integrations
â”‚   â”‚   â”œâ”€â”€ llm/                # OpenRouter client
â”‚   â”‚   â”œâ”€â”€ search/             # SearxNG, SerperDev, Perplexity
â”‚   â”‚   â”œâ”€â”€ crawl/              # Crawl4AI integration
â”‚   â”‚   â””â”€â”€ extraction/         # Dockling processor
â”‚   â”œâ”€â”€ database/                # PostgreSQL + pgvector
â”‚   â”‚   â”œâ”€â”€ models.py           # SQLAlchemy models
â”‚   â”‚   â””â”€â”€ repositories/       # Data access layer
â”‚   â”œâ”€â”€ rag/                     # RAG system
â”‚   â”‚   â”œâ”€â”€ embeddings.py       # Sentence-Transformers
â”‚   â”‚   â””â”€â”€ vector_store.py     # pgvector operations
â”‚   â”œâ”€â”€ core/                    # Configuration & pipelines
â”‚   â”‚   â”œâ”€â”€ config.py           # Settings
â”‚   â”‚   â””â”€â”€ circuit_breaker.py  # Fault tolerance
â”‚   â””â”€â”€ utils/                   # Utilities
â”œâ”€â”€ tests/                       # Test suite (TDD)
â”‚   â”œâ”€â”€ unit/                   # Fast, isolated tests
â”‚   â”œâ”€â”€ integration/            # DB, external services
â”‚   â””â”€â”€ e2e/                    # Full pipeline tests
â”œâ”€â”€ docs/                        # Documentation
â”œâ”€â”€ alembic/                     # Database migrations
â”œâ”€â”€ streamlit_ui.py             # Streamlit interface
â”œâ”€â”€ docker-compose.yml          # Container orchestration
â”œâ”€â”€ Dockerfile                  # API container
â”œâ”€â”€ Dockerfile.streamlit        # UI container
â””â”€â”€ requirements.txt            # Python dependencies
```

### Service Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit UI   â”‚â”€â”€â”€â”€â–¶â”‚   FastAPI API    â”‚â”€â”€â”€â”€â–¶â”‚ PostgreSQL +    â”‚
â”‚   (Port 8501)   â”‚     â”‚   (Port 8001)    â”‚     â”‚   pgvector      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â”œâ”€â”€â”€â”€â–¶ Redis (Cache)
                               â”‚
                               â”œâ”€â”€â”€â”€â–¶ OpenRouter (LLM)
                               â”‚
                               â”œâ”€â”€â”€â”€â–¶ Langfuse (Monitoring)
                               â”‚
                               â””â”€â”€â”€â”€â–¶ Search Engines
                                     â”œâ”€ SearxNG (Primary)
                                     â”œâ”€ SerperDev (Fallback)
                                     â””â”€ Perplexity (Optional)
```

---

## ğŸ› ï¸ Technology Stack

| Category | Technologies |
|----------|-------------|
| **Backend** | FastAPI 0.115.5, Python 3.11, Pydantic AI 0.0.14 |
| **Database** | PostgreSQL 16 + pgvector 0.3.6 |
| **Cache** | Redis 7 (redis-py 5.2.0) |
| **LLM** | OpenRouter (Claude 3.5 Sonnet, DeepSeek R1) via openai 1.55.3 |
| **Monitoring** | Langfuse 2.56.0 |
| **Document Processing** | Dockling 2.14.0 |
| **Web Crawling** | Crawl4AI 0.4.249 |
| **Embeddings** | Sentence-Transformers 3.3.1 (384D vectors) |
| **Search** | SearxNG, SerperDev, Perplexity |
| **Testing** | pytest 8.3.4, pytest-asyncio 0.24.0 |
| **Code Quality** | Ruff 0.8.1, mypy 1.13.0 |
| **Frontend** | Streamlit (for testing/admin UI) |

---

## ğŸ’» Usage

### Via Streamlit UI (Recommended)

1. **Open**: http://localhost:8501
2. **Select Mode**:
   - **Search**: Fast results (30-60s) with multi-source aggregation
   - **Research**: Deep analysis (2-5 min) with iterative refinement
   - **Document Library**: Upload and query documents
3. **Enter Query** and configure parameters
4. **View Results** with citations, sources, and quality metrics
5. **Export** as JSON or Markdown

### Via API

#### Search Endpoint

```bash
curl -X POST http://localhost:8001/api/v1/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is Pydantic AI?",
    "max_sources": 20,
    "timeout": 60,
    "prompt_strategy": "auto"
  }'
```

**Response:**
```json
{
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "answer": "Pydantic AI is...",
  "sources": [...],
  "grounding_score": 0.87,
  "execution_time": 45.2
}
```

#### Research Endpoint

```bash
curl -X POST http://localhost:8001/api/v1/research \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Explain AI agents and their applications",
    "max_iterations": 3,
    "timeout": 300,
    "mode": "balanced",
    "prompt_strategy": "dynamic"
  }'
```

#### Document Upload & Query

```bash
# Upload document
curl -X POST http://localhost:8001/api/v1/documents/upload \
  -F "file=@document.pdf" \
  -F "collection_name=research_papers"

# Query documents
curl -X POST http://localhost:8001/api/v1/documents/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What are the main findings?",
    "collection_name": "research_papers",
    "top_k": 5
  }'
```

---

## âš™ï¸ Configuration

### Environment Variables

Key settings in `.env`:

```bash
# Database
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=research_db
DATABASE_URL=postgresql+asyncpg://user:pass@localhost:5432/research_db

# Redis
REDIS_URL=redis://localhost:6379/0

# LLM Configuration (Cost-Optimized with FREE models)
RESEARCH_LLM_MODEL=deepseek/deepseek-r1:free          # Primary (FREE)
FALLBACK_LLM_MODEL=google/gemini-2.0-flash-thinking-exp:free  # Fallback (FREE)
RESEARCH_LLM_TEMPERATURE=0.3
RESEARCH_LLM_MAX_TOKENS=8192

# Search Services
SEARXNG_BASE_URL=http://localhost:8080
SERPER_API_KEY=your_key
PERPLEXITY_API_KEY=your_key  # Optional

# Search Configuration
ENABLE_SEARCH_FALLBACK=true
SEARXNG_MIN_RESULTS_THRESHOLD=3
SEARCH_MAX_SOURCES=20
RESEARCH_MAX_SOURCES=80

# Feature Flags
ENABLE_DYNAMIC_PROMPTS=true   # Query-aware prompts (no LLM cost)
ENABLE_LLM_FALLBACK=true      # Fallback to Gemini if DeepSeek fails

# RAG Configuration
RAG_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
RAG_VECTOR_DIMENSION=384
RAG_SIMILARITY_THRESHOLD=0.3
RAG_TOP_K=20

# Performance
RESEARCH_MAX_ITERATIONS=3
RESEARCH_TIMEOUT_SECONDS=300
API_RATE_LIMIT=10/minute
```

### Ports

| Service | Internal | External | Configurable |
|---------|----------|----------|--------------|
| Research API | 8000 | 8001 | Yes (API_PORT) |
| Streamlit UI | 8501 | 8501 | Yes |
| PostgreSQL | 5432 | 5433 | Yes |
| Redis | 6379 | 6380 | Yes |
| SearxNG | 8080 | 8080 | No (external) |

---

## ğŸ§‘â€ğŸ’» Development

### Local Setup

```bash
# Clone repository
cd research-service

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
pip install -r requirements-dev.txt

# Configure environment
cp .env.example .env
# Edit .env with your API keys

# Start infrastructure
docker-compose up -d postgres redis

# Run database migrations
alembic upgrade head
```

### Development Workflow

```bash
# Run linting
ruff check src/ tests/

# Fix linting issues
ruff check --fix src/ tests/

# Format code
ruff format src/ tests/

# Type checking
mypy src/

# Run tests
pytest tests/unit/ -v
pytest tests/integration/ -v
pytest tests/ --cov=src --cov-report=html

# Start API (development)
uvicorn src.api.main:app --reload --port 8001

# Start Streamlit UI
streamlit run streamlit_ui.py
```

### Test-Driven Development (TDD)

**Workflow: RED â†’ GREEN â†’ REFACTOR**

1. âœ… Write failing test first
2. âœ… Write minimal code to pass
3. âœ… Refactor while keeping tests green

**Coverage Requirements:**
- Minimum: **80%** overall
- Critical paths: **100%**
- Edge cases: **Mandatory**

---

## ğŸ§ª Testing

### Test Structure

```
tests/
â”œâ”€â”€ unit/                    # Fast, isolated tests
â”‚   â”œâ”€â”€ test_config.py
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ test_search_agent.py
â”‚   â”‚   â””â”€â”€ test_research_agent.py
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ test_llm_client.py
â”‚       â””â”€â”€ test_document_service.py
â”œâ”€â”€ integration/             # DB, external services
â”‚   â”œâ”€â”€ test_database.py
â”‚   â””â”€â”€ test_rag_pipeline.py
â””â”€â”€ e2e/                     # Full pipeline tests
    â”œâ”€â”€ test_search_pipeline.py
    â””â”€â”€ test_research_pipeline.py
```

### Running Tests

```bash
# All tests
pytest tests/ -v

# Unit tests only
pytest tests/unit/ -v

# With coverage
pytest tests/unit/ --cov=src --cov-report=html

# Specific test file
pytest tests/unit/agents/test_search_agent.py -v

# Test markers
pytest -m "unit and fast" -v        # Fast unit tests
pytest -m "integration" -v          # Integration tests
pytest -m "e2e and slow" -v         # E2E tests

# View coverage report
open htmlcov/index.html
```

### Test Fixtures

Key fixtures in `conftest.py`:
- `async_db_session` - Database session
- `mock_llm_client` - Mocked LLM
- `mock_search_client` - Mocked search
- `sample_documents` - Test documents

---

## ğŸ“– API Reference

### Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/v1/health` | Health check |
| POST | `/api/v1/search` | Fast search (30-60s) |
| POST | `/api/v1/research` | Deep research (2-5 min) |
| GET | `/api/v1/sessions/{id}` | Get session details |
| POST | `/api/v1/documents/upload` | Upload document |
| POST | `/api/v1/documents/query` | Query documents |
| GET | `/api/v1/documents` | List documents |
| DELETE | `/api/v1/documents/{id}` | Delete document |

### Interactive Documentation

- **Swagger UI**: http://localhost:8001/api/docs
- **ReDoc**: http://localhost:8001/api/redoc
- **OpenAPI JSON**: http://localhost:8001/api/openapi.json

---

## ğŸš¢ Deployment

### Docker Production Build

```bash
# Build images
docker-compose -f docker-compose.yml build

# Start all services
docker-compose up -d

# View logs
docker-compose logs -f research-api

# Scale API
docker-compose up -d --scale research-api=3

# Stop services
docker-compose down
```

### Environment-Specific Configs

```bash
# Development
docker-compose up -d

# Production (with resource limits)
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d
```

### Health Checks

```bash
# API health
curl http://localhost:8001/api/v1/health

# Database connection
docker-compose exec postgres pg_isready

# Redis connection
docker-compose exec redis redis-cli ping
```

---

## ğŸ“Š Project Status

### âœ… Completed Features (Production Ready)

| Phase | Features | Status | Tests | Coverage |
|-------|----------|--------|-------|----------|
| **Phase 1** | Foundation (DB, LLM, RAG) | âœ… COMPLETE | 134 | 85.20% |
| **Phase 2** | Agents (Search, Research) | âœ… COMPLETE | 36 | 94%+ |
| **Phase 3** | Citation Grounding | âœ… COMPLETE | 8 | 98%+ |
| **Phase 4** | Configurable Prompts | âœ… COMPLETE | 9 | 95%+ |
| **Phase 5** | Document Upload/RAG | âœ… COMPLETE | - | - |
| **Perplexity** | Fallback Search | âœ… COMPLETE | 29 | 98%+ |
| **Reranking** | Semantic Reranking | âœ… COMPLETE | - | - |

### ğŸ¯ Current Implementation

**Core Services:**
- âœ… Multi-agent system (SearchAgent, ResearchAgent)
- âœ… RAG system with pgvector
- âœ… Document processing (PDF, Word, Excel)
- âœ… Web crawling with Crawl4AI
- âœ… Citation grounding & hallucination detection
- âœ… Multi-source synthesis
- âœ… Content-type adaptation
- âœ… Configurable prompt strategies
- âœ… Document library (upload, query, manage)
- âœ… Streamlit UI with 3 modes
- âœ… Perplexity fallback search
- âœ… Semantic reranking

**Search Engines:**
- âœ… SearxNG (primary)
- âœ… SerperDev (fallback)
- âœ… Perplexity (optional fallback with circuit breaker)

**Bug Fixes:**
- âœ… SearxNG NoneType error handling (4 locations)
- âœ… ResearchAgent logger initialization fix

### ğŸ“‹ TODO

- [ ] Task 4: Update Vector Store for Collections
- [ ] Task 6: Write Unit Tests for Document Service
- [ ] Task 7: End-to-End Testing
- [ ] Production deployment configuration
- [ ] Horizontal scaling with job queue
- [ ] Advanced RAG features (hybrid search, re-ranking refinements)

### Performance Metrics

- Search mode: **30-60s** (target <60s p95) âœ…
- Research mode: **2-5 min** (target <5min p95) âœ…
- Test coverage: **85%+** (target â‰¥80%) âœ…
- API response: **<200ms** (excluding pipeline) âœ…

---

## ğŸ“š Additional Documentation

- **Streamlit UI Specification**: `docs/STREAMLIT_APP_SPEC.md`
- **Process Flows**: `docs/PROCESS_FLOWS.md`
- **Architecture Details**: `docs/STREAMLIT_ARCHITECTURE.md`
- **API Specification**: `docs/API_SPECIFICATION.md`

---

## ğŸ¤ Relationship with Existing Codebase

The existing `simple_perplexica` code in `../src/` and `../services/` is **completely untouched**:
- Original search endpoints continue to work
- No shared code or dependencies
- Research service runs independently on port 8001
- Can reuse existing SearxNG/SerperDev configuration

---

## ğŸ“ License

See parent project license.

---

## ğŸ™‹ Support

- **Issues**: GitHub Issues
- **Documentation**: `docs/` directory
- **API Docs**: http://localhost:8001/api/docs (when running)
- **Monitoring**: Langfuse dashboard at https://cloud.langfuse.com

---

**Last Updated**: November 13, 2025  
**Version**: 1.0.0  
**Status**: Production Ready
