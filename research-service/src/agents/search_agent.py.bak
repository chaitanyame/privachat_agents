"""SearchAgent implementation using Pydantic AI.

The SearchAgent coordinates intelligent search workflows:
- Query decomposition (complex ‚Üí focused sub-queries)
- Search coordination (parallel multi-source execution)
- Result ranking (relevance scoring and quality filtering)
- Source deduplication and validation

Architecture:
    Uses Pydantic AI Agent framework with:
    - Dependency injection for LLM, DB, HTTP clients
    - Structured output validation via Pydantic models
    - Tool-based workflow (decompose ‚Üí coordinate ‚Üí rank)
    - Langfuse tracing for observability
"""

from __future__ import annotations

import asyncio
from dataclasses import dataclass
from typing import Any

import structlog

logger = structlog.get_logger(__name__)

import json
import httpx
from pydantic import BaseModel, Field
from pydantic_ai import ModelRetry
from sqlalchemy.ext.asyncio import AsyncSession

from src.core.search_modes import SearchMode, get_mode_from_string
from src.core.config import settings
from src.services.crawl.crawl4ai_client import Crawl4AIClient
from src.services.document.dockling_processor import DocklingProcessor
from src.services.embedding.embedding_service import EmbeddingService
from src.services.embedding.semantic_reranker import (
    RecencyConfig,
    RerankingConfig,
    SemanticReranker,
)
from src.services.llm.langfuse_tracer import LangfuseTracer
from src.services.llm.openrouter_client import OpenRouterClient
from src.services.search.searxng_client import SearxNGClient
from src.utils.language_detector import detect_language, get_language_name
from src.utils.query_normalizer import normalize_query
from src.utils.temporal_validator import TemporalValidator

# =============================================================================
# Pydantic Models for Structured Output
# =============================================================================


class SubQuery(BaseModel):
    """Decomposed sub-query with intent and priority.

    Attributes:
        query: Focused sub-query text (1-200 chars)
        intent: Query intent classification (definition/factual/opinion)
        priority: Execution priority (1=highest, 5=lowest)
        temporal_scope: Time filter (recent/past_year/past_month/any)
        specific_year: Specific year mentioned (e.g., 2022, 2023) - overrides temporal_scope
        language: Query language ISO code (en/es/fr/de) for region-specific results
    """

    query: str = Field(..., min_length=1, max_length=200)
    intent: str = Field(..., pattern=r"^(definition|factual|opinion)$")
    priority: int = Field(ge=1, le=5)
    temporal_scope: str = Field(
        default="any",
        pattern=r"^(recent|past_year|past_month|past_week|any)$",
        description="Time filter: recent (past month), past_year, past_month, past_week, any"
    )
    specific_year: int | None = Field(
        default=None,
        ge=1990,
        le=2030,
        description="Specific year if mentioned (e.g., 2022, 2023). Overrides temporal_scope."
    )
    language: str = Field(
        default="en",
        pattern=r"^(en|es|fr|de)$",
        description="Query language: en (English), es (Spanish), fr (French), de (German)"
    )


class SearchSource(BaseModel):
    """Search result source with metadata.

    Attributes:
        title: Source title
        url: Source URL
        snippet: Text excerpt/snippet from search results
        content: Full extracted content from URL (populated by crawling)
        relevance: Relevance score from search API (0.0-1.0)
        semantic_score: Cross-encoder reranking score (0.0-1.0, optional)
        final_score: Combined score for ranking (0.0-1.0)
        source_type: Source classification (web/academic/news)
    """

    title: str
    url: str
    snippet: str
    content: str | None = None  # Full content from crawling
    relevance: float = Field(ge=0.0, le=1.0)
    semantic_score: float | None = Field(default=None, ge=0.0, le=1.0)
    final_score: float = Field(default=0.0, ge=0.0, le=1.0)
    source_type: str = Field(pattern=r"^(web|academic|news)$")
    
    def model_post_init(self, __context) -> None:
        """Initialize final_score if not provided."""
        if self.final_score == 0.0:
            self.final_score = self.relevance


class QueryDecomposition(BaseModel):
    """Result of query decomposition.
    
    Attributes:
        sub_queries: List of decomposed sub-queries
    """
    sub_queries: list[SubQuery] = Field(
        ..., 
        min_length=1, 
        max_length=5,
        description="1-5 focused sub-queries"
    )


class SearchOutput(BaseModel):
    """Final search output with all results and metadata.

    Attributes:
        answer: AI-generated answer synthesizing the sources
        sub_queries: List of decomposed sub-queries
        sources: Ranked and filtered search sources
        execution_time: Total execution time in seconds
        confidence: Overall confidence score (0.0-1.0)
    """

    answer: str = Field(..., min_length=1, description="AI-generated answer")
    sub_queries: list[SubQuery]
    sources: list[SearchSource]
    execution_time: float
    confidence: float = Field(ge=0.0, le=1.0)


# =============================================================================
# Utility Functions
# =============================================================================


def extract_json_from_markdown(content: str) -> str:
    """Extract JSON from markdown code blocks.
    
    Some LLM models wrap JSON responses in ```json...``` or ```...``` blocks.
    This function extracts the JSON content from such blocks.
    
    Args:
        content: Raw LLM response text
        
    Returns:
        Cleaned JSON string
    """
    if not content.startswith("```"):
        return content
        
    lines = content.split("\n")
    json_lines = []
    in_code_block = False
    
    for line in lines:
        if line.strip().startswith("```"):
            in_code_block = not in_code_block
            continue
        if in_code_block:
            json_lines.append(line)
    
    extracted = "\n".join(json_lines).strip()
    logger.info(f"üìù Extracted JSON from markdown code block: {len(extracted)} chars")
    return extracted


# =============================================================================
# Dependency Injection
# =============================================================================


@dataclass
class SearchAgentDeps:
    """SearchAgent dependencies for dependency injection.

    Attributes:
        llm_client: OpenRouter LLM client for AI calls
        tracer: Langfuse tracer for observability
        db: Async SQLAlchemy database session
        searxng_client: HTTP client for SearxNG API
        serperdev_api_key: API key for SerperDev service
        crawl_client: Crawl4AI client for URL content extraction
        embedding_service: Embedding service for semantic reranking
        max_sources: Maximum number of sources to return (default: 20)
        timeout: Search timeout in seconds (default: 60.0)
        enable_crawling: Whether to crawl URLs for full content (default: True)
        max_crawl_urls: Maximum URLs to crawl for content (default: 5)
        enable_reranking: Whether to use semantic reranking (default: True)
        rerank_weight: Weight for semantic score in final ranking (default: 0.6)
        enable_diversity_penalty: Enable diversity penalty to reduce duplicates (default: False)
        enable_recency_boost: Enable recency boost for temporal queries (default: False)
        enable_query_aware: Enable query-aware score adaptations (default: False)
        reranking_config: Advanced reranking configuration (optional)
    """

    llm_client: OpenRouterClient
    tracer: LangfuseTracer
    db: AsyncSession
    # Accept either raw httpx.AsyncClient or SearxNGClient abstraction
    searxng_client: Any
    serperdev_api_key: str
    crawl_client: Crawl4AIClient
    document_processor: DocklingProcessor
    embedding_service: EmbeddingService
    max_sources: int = 20
    timeout: float = 60.0
    min_sources: int = 5  # Minimum required sources for valid output
    min_confidence: float = 0.5  # Minimum confidence threshold
    enable_crawling: bool = True  # Enable URL crawling
    max_crawl_urls: int = 5  # Maximum URLs to crawl
    enable_reranking: bool = True  # Enable semantic reranking
    rerank_weight: float = 0.6  # Weight for semantic score (0.0-1.0)
    # Enhanced reranking configuration
    enable_diversity_penalty: bool = False  # Enable diversity penalty (deduplication)
    enable_recency_boost: bool = False  # Enable recency boost for temporal queries
    enable_query_aware: bool = False  # Enable query-aware score adaptations
    reranking_config: RerankingConfig | None = None  # Advanced reranking config


# =============================================================================
# SearchAgent Implementation
# =============================================================================


class SearchAgent:
    """Intelligent search coordination agent using Pydantic AI.

    The SearchAgent orchestrates complex search workflows by:
    1. Decomposing queries into focused sub-queries
    2. Coordinating parallel searches across multiple sources
    3. Ranking results by relevance and quality
    4. Validating output meets quality criteria

    Example:
        >>> deps = SearchAgentDeps(
        ...     llm_client=client,
        ...     tracer=tracer,
        ...     db=session,
        ...     searxng_client=http_client,
        ...     serperdev_api_key="key"
        ... )
        >>> agent = SearchAgent(deps=deps)
        >>> result = await agent.run("What are AI agents?")
        >>> print(f"Found {len(result.sources)} sources")
    """

    def __init__(self, deps: SearchAgentDeps) -> None:
        """Initialize SearchAgent with dependencies.

        Args:
            deps: SearchAgentDeps with all required dependencies
        """
        self.deps = deps
        self.max_sources = deps.max_sources
        self.timeout = deps.timeout
        self.temporal_validator = TemporalValidator()

        # Log search configuration
        # SearxNG is now the primary search backend; SerperDev acts as a fallback when available
        if deps.serperdev_api_key and deps.serperdev_api_key.strip():
            logger.info("SearchAgent initialized with SearxNG (primary) + SerperDev (fallback)")
        else:
            logger.info("SearchAgent initialized with SearxNG (primary, no SerperDev key available)")
        
        logger.info(f"‚úÖ Temporal validation enabled with post-retrieval filtering")

    async def decompose_query(self, query: str) -> list[SubQuery]:
        """Decompose complex query into focused sub-queries using Pydantic AI.

        Args:
            query: Complex user query to decompose

        Returns:
            List of SubQuery objects with intent and priority

        Example:
            >>> sub_queries = await agent.decompose_query(
            ...     "What are AI agents and how do they work?"
            ... )
            >>> print(len(sub_queries))
            2
        """
        logger.info(f"üß© QUERY DECOMPOSITION START")
        logger.info(f"üì• Input query (raw): '{query}'")
        
        # Normalize query for consistency
        normalized_query = normalize_query(query)
        logger.info(f"üì• Input query (normalized): '{normalized_query}'")
        
        # Detect language for better search results
        detected_lang = detect_language(query)
        lang_name = get_language_name(detected_lang)
        logger.info(f"üåç Detected language: {lang_name} ({detected_lang})")

        # Use OpenRouter LLM directly with JSON mode for structured decomposition
        system_prompt = """You are a query decomposition expert. Break down user queries into focused sub-queries.

Guidelines:
- Simple queries (1 topic): Return 1-2 sub-queries
- Complex queries (multiple topics): Return 2-4 sub-queries  
- Each sub-query should be specific and searchable
- Intent types: "factual" (facts/data), "definition" (what is X), "opinion" (views/analysis)
- Priority: 1 (most important) to 5 (least important)

Temporal Detection (CRITICAL - Extract ANY year mentioned):
- If query mentions SPECIFIC YEAR (2022, 2023, 2024, etc.):
  * Set "specific_year": [year as integer]
  * Keep that year in sub-query text
  * Set temporal_scope to "any"
  
- If query has "latest"/"recent"/"new" (no specific year):
  * Set temporal_scope: "recent" 
  * Add current year (2025) to sub-query
  * Set specific_year: null
  
- If query has "past month"/"last month":
  * Set temporal_scope: "past_month"
  * Set specific_year: null
  
- If query has "past week"/"this week":
  * Set temporal_scope: "past_week"
  * Set specific_year: null
  
- If query has "past year":
  * Set temporal_scope: "past_year"
  * Set specific_year: null
  
- No time keywords:
  * Set temporal_scope: "any"
  * Set specific_year: null

Examples:

Query: "What are AI agents?"
Response:
{
  "sub_queries": [
    {"query": "What are AI agents?", "intent": "definition", "priority": 1, "temporal_scope": "any", "specific_year": null}
  ]
}

Query: "Recent news about Microsoft Azure"
Response:
{
  "sub_queries": [
    {"query": "Microsoft Azure recent news 2025", "intent": "factual", "priority": 1, "temporal_scope": "recent", "specific_year": null}
  ]
}

Query: "GitHub Universe 2023 announcements"
Response:
{
  "sub_queries": [
    {"query": "GitHub Universe 2023 announcements", "intent": "factual", "priority": 1, "temporal_scope": "any", "specific_year": 2023}
  ]
}

Query: "AI developments in 2022"
Response:
{
  "sub_queries": [
    {"query": "AI developments 2022", "intent": "factual", "priority": 1, "temporal_scope": "any", "specific_year": 2022}
  ]
}

Query: "Python trends 2020 vs 2024"
Response:
{
  "sub_queries": [
    {"query": "Python trends 2020", "intent": "factual", "priority": 1, "temporal_scope": "any", "specific_year": 2020},
    {"query": "Python trends 2024", "intent": "factual", "priority": 1, "temporal_scope": "any", "specific_year": 2024}
  ]
}

CRITICAL RULES:
1. ALWAYS extract specific years (2022, 2023, 2024, etc.) and set specific_year field
2. KEEP the year in the sub-query text (don't remove it)
3. specific_year overrides temporal_scope for search filtering
4. Only use "recent" temporal_scope when NO specific year mentioned

Respond with valid JSON only."""

        try:
            import json
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Query: {query}\n\nDecompose this into sub-queries."}
            ]
            
            # ============ CRITICAL LOG: LLM INPUT ============
            logger.info(f"ü§ñ LLM CALL: Query Decomposition")
            logger.info(f"üì§ Model: meta-llama/llama-4-maverick:free")
            logger.info(f"üì§ Temperature: 0.3, Max Tokens: 500")
            logger.info(f"üì§ Messages count: {len(messages)}")
            logger.info(f"üì§ System prompt length: {len(system_prompt)} chars")
            logger.info(f"üì§ User message: '{messages[1]['content'][:200]}...'")
            
            # Use Llama 4 Maverick (free model) via OpenRouter
            # Note: OpenRouterClient is initialized with a default model,
            # but we can override by temporarily changing it
            original_model = self.deps.llm_client.model
            self.deps.llm_client.model = "meta-llama/llama-4-maverick:free"  # FREE tier
            
            # Meta models don't support response_format parameter
            # We rely on the prompt instructing JSON output instead
            response = await self.deps.llm_client.chat(
                messages=messages,
                temperature=0.3,
                max_tokens=500,
            )
            
            # Restore original model
            self.deps.llm_client.model = original_model
            
            # ============ CRITICAL LOG: LLM RESPONSE ============
            content = response["content"].strip()
            logger.info(f"ÔøΩ LLM RESPONSE received")
            logger.info(f"üì• Response length: {len(content)} chars")
            logger.info(f"üì• Raw response:\n{content}")
            
            # Extract JSON from markdown code blocks if present
            content = extract_json_from_markdown(content)
            
            # Parse JSON response
            data = json.loads(content)
            logger.info(f"‚úÖ JSON parsing successful")
            logger.info(f"‚úÖ Parsed data keys: {list(data.keys())}")
            
            # Validate and convert to SubQuery objects
            decomposition = QueryDecomposition(**data)
            
            # Add detected language to all sub-queries
            for sq in decomposition.sub_queries:
                sq.language = detected_lang
            
            logger.info(f"üéØ DECOMPOSITION COMPLETE: {len(decomposition.sub_queries)} sub-queries")
            
            for i, sq in enumerate(decomposition.sub_queries, 1):
                logger.info(
                    f"  [{i}] Query: '{sq.query}' | "
                    f"Intent: {sq.intent} | "
                    f"Priority: {sq.priority} | "
                    f"Temporal: {sq.temporal_scope} | "
                    f"Year: {sq.specific_year or 'N/A'} | "
                    f"Lang: {sq.language}"
                )
            
            return decomposition.sub_queries
            
        except Exception as e:
            logger.error(f"‚ùå Query decomposition failed: {e}, using fallback")
            # Fallback: treat as single factual query
            
            # Extract specific year from query (2020-2030 range)
            import re
            year_pattern = r'\b(20[2-3][0-9])\b'  # Matches 2020-2039
            year_match = re.search(year_pattern, query)
            specific_year = int(year_match.group(1)) if year_match else None
            
            # Detect temporal keywords
            temporal_keywords = ["latest", "recent", "new", "now", "current", "today"]
            has_temporal_intent = any(kw in query.lower() for kw in temporal_keywords)
            
            # Determine temporal scope (specific year takes priority)
            if specific_year:
                temporal_scope = "any"  # Year filtering handles it
                logger.info(f"  Fallback detected specific year: {specific_year}")
            elif has_temporal_intent:
                temporal_scope = "recent"
                logger.info(f"  Fallback detected temporal keywords: recent")
            else:
                temporal_scope = "any"
                logger.info(f"  Fallback: no temporal filtering")
            
            return [SubQuery(
                query=query,
                intent="factual",
                priority=1,
                temporal_scope=temporal_scope,
                specific_year=specific_year,
                language=detected_lang
            )]

    async def coordinate_search(self, sub_queries: list[SubQuery]) -> list[SearchSource]:
        """Coordinate parallel searches across multiple sources.

        Args:
            sub_queries: List of sub-queries to search

        Returns:
            Combined and deduplicated search results

        Raises:
            httpx.TimeoutException: Handled gracefully, returns partial results

        Example:
            >>> sources = await agent.coordinate_search(sub_queries)
            >>> print(f"Found {len(sources)} unique sources")
        """
        logger.info(f"üîÑ Coordinating search for {len(sub_queries)} sub-queries")
        for i, sq in enumerate(sub_queries, 1):
            logger.info(f"  [{i}] {sq.query}")

        results: list[SearchSource] = []
        seen_urls: set[str] = set()

        # Execute searches in parallel
        search_tasks = [self._search_source(sq) for sq in sub_queries]

        try:
            logger.info(f"‚è≥ Executing {len(search_tasks)} search tasks in parallel...")
            search_results = await asyncio.gather(*search_tasks, return_exceptions=True)

            logger.info(f"üì• Got {len(search_results)} search results back")

            # Flatten and deduplicate results
            for idx, result in enumerate(search_results):
                if isinstance(result, Exception):
                    logger.error(f"  Search task {idx+1} failed with exception: {result}")
                elif isinstance(result, list):
                    logger.info(f"  Search task {idx+1} returned {len(result)} sources")
                    for source in result:
                        if source.url not in seen_urls:
                            seen_urls.add(source.url)
                            results.append(source)
                else:
                    logger.warning(f"  Search task {idx+1} returned unexpected type: {type(result)}")

        except Exception as e:
            # Gracefully handle timeouts/errors, return partial results
            logger.error(f"‚ùå Exception during search coordination: {e}")

        logger.info(f"‚úÖ Coordination complete: {len(results)} unique sources found")
        return results

    async def _search_source(self, sub_query: SubQuery) -> list[SearchSource]:
        """Search using SearxNG (primary) with fallback to SerperDev.

        Order:
            1. Attempt SearxNG (preferred per spec)
            2. If SearxNG fails or returns no usable results, try SerperDev (if key)
        """
        # -----------------------------
        # Primary: SearxNG
        # -----------------------------
        logger.info("üåê SearxNG primary search", query=sub_query.query)
        try:
            # Support both wrapper client and raw httpx.AsyncClient
            if isinstance(self.deps.searxng_client, SearxNGClient):
                # Map temporal_scope to SearxNG time_range when possible
                time_range_map = {
                    "past_week": "week",
                    "past_month": "month",
                    "recent": "month",
                    "past_year": "year",
                }
                time_range = None
                if sub_query.specific_year is not None:
                    # SearxNG does not have direct year filter; leave None (handled downstream)
                    time_range = None
                else:
                    time_range = time_range_map.get(sub_query.temporal_scope)
                # First attempt
                categories = getattr(settings, "SEARXNG_DEFAULT_CATEGORIES", ["general"])
                engines = getattr(settings, "SEARXNG_DEFAULT_ENGINES", []) or None
                searx_results = await self.deps.searxng_client.search(
                    sub_query.query,
                    limit=10,
                    categories=categories,
                    engines=engines,
                    language=sub_query.language,
                    time_range=time_range,
                    safesearch=getattr(settings, "SEARXNG_SAFESEARCH", 1),
                )
                if searx_results:
                    logger.info("‚úÖ SearxNG success (primary attempt)", count=len(searx_results))
                    return [
                        SearchSource(
                            title=r.get("title", ""),
                            url=r.get("url", ""),
                            snippet=r.get("content", ""),
                            relevance=0.7,
                            source_type="web",
                        )
                        for r in searx_results
                    ]
                logger.warning("SearxNG returned 0 results (primary); evaluating retry conditions")

                # Year heuristic attempt: if a specific year exists, try broad 'year' range
                if sub_query.specific_year is not None:
                    logger.info("üîÅ SearxNG retry (year heuristic)", year=sub_query.specific_year)
                    searx_year_results = await self.deps.searxng_client.search(
                        sub_query.query,
                        limit=10,
                        categories=categories,
                        engines=engines,
                        language=sub_query.language,
                        time_range="year",
                        safesearch=getattr(settings, "SEARXNG_SAFESEARCH", 1),
                    )
                    if searx_year_results:
                        logger.info("‚úÖ SearxNG success (year heuristic)", count=len(searx_year_results))
                        return [
                            SearchSource(
                                title=r.get("title", ""),
                                url=r.get("url", ""),
                                snippet=r.get("content", ""),
                                relevance=0.7,
                                source_type="web",
                            )
                            for r in searx_year_results
                        ]
                    else:
                        logger.warning("SearxNG year heuristic yielded 0 results")

                # Expanded retry: drop language restriction & add broader categories if missing
                expanded_categories = sorted(set(list(categories) + ["general", "news"]))
                logger.info("üîÅ SearxNG retry (expanded scope)", categories=expanded_categories)
                searx_retry_results = await self.deps.searxng_client.search(
                    sub_query.query,
                    limit=10,
                    categories=expanded_categories,
                    engines=engines,
                    language=None,  # remove language filter to broaden
                    time_range=None,
                    safesearch=getattr(settings, "SEARXNG_SAFESEARCH", 1),
                )
                if searx_retry_results:
                    logger.info("‚úÖ SearxNG success (expanded retry)", count=len(searx_retry_results))
                    return [
                        SearchSource(
                            title=r.get("title", ""),
                            url=r.get("url", ""),
                            snippet=r.get("content", ""),
                            relevance=0.65,  # Slightly lower relevance due to broadened scope
                            source_type="web",
                        )
                        for r in searx_retry_results
                    ]
                else:
                    logger.warning("SearxNG expanded retry returned 0 results; proceeding to fallback")
            else:  # Raw httpx client path
                # Raw client path - build params manually
                params = {
                    "q": sub_query.query,
                    "format": "json",
                    "language": sub_query.language,
                }
                # Temporal mapping as above
                time_range_map = {
                    "past_week": "week",
                    "past_month": "month",
                    "recent": "month",
                    "past_year": "year",
                }
                if sub_query.specific_year is None:
                    tr = time_range_map.get(sub_query.temporal_scope)
                    if tr:
                        params["time_range"] = tr
                cats = getattr(settings, "SEARXNG_DEFAULT_CATEGORIES", ["general"])
                if cats:
                    params["categories"] = ",".join(cats)
                engines = getattr(settings, "SEARXNG_DEFAULT_ENGINES", [])
                if engines:
                    params["engines"] = ",".join(engines)
                params["safesearch"] = getattr(settings, "SEARXNG_SAFESEARCH", 1)

                response = await self.deps.searxng_client.get("/search", params=params, timeout=self.timeout)
                logger.info("SearxNG response status", status=response.status_code)
                if response.status_code == 200:
                    data = response.json()
                    results = data.get("results", [])
                    if results:
                        logger.info("‚úÖ SearxNG success", count=len(results))
                        return [
                            SearchSource(
                                title=r.get("title", ""),
                                url=r.get("url", ""),
                                snippet=r.get("content", ""),
                                relevance=0.7,
                                source_type="web",
                            )
                            for r in results
                        ]
                    else:
                        logger.warning("SearxNG returned 0 results; will consider fallback")
                else:
                    logger.warning("SearxNG non-200", status=response.status_code)
        except httpx.TimeoutException:
            logger.warning("SearxNG timeout", query=sub_query.query)
        except httpx.ConnectError as e:
            logger.error("SearxNG connect error", error=str(e))
        except Exception as e:
            logger.error("SearxNG unexpected error", error=str(e))

        # -----------------------------
        # Fallback: SerperDev
        # -----------------------------
        if not (self.deps.serperdev_api_key and self.deps.serperdev_api_key.strip()):
            logger.warning("No SerperDev API key available; returning empty results")
            return []

        temporal_info = f"temporal: {sub_query.temporal_scope}"
        if sub_query.specific_year:
            temporal_info += f", year: {sub_query.specific_year}"
        logger.info("‚Ü©Ô∏è Fallback to SerperDev", query=sub_query.query, temporal=temporal_info)

        try:
            search_params = {
                "q": sub_query.query,
                "num": 10,
                "gl": sub_query.language,
            }
            if sub_query.specific_year:
                year = sub_query.specific_year
                search_params["tbs"] = f"cdr:1,cd_min:1/1/{year},cd_max:12/31/{year}"
            elif sub_query.temporal_scope == "past_week":
                search_params["tbs"] = "qdr:w"
            elif sub_query.temporal_scope in {"past_month", "recent"}:
                search_params["tbs"] = "qdr:m"
            elif sub_query.temporal_scope == "past_year":
                search_params["tbs"] = "qdr:y"

            async with httpx.AsyncClient() as client:
                response = await client.post(
                    "https://google.serper.dev/search",
                    headers={
                        "X-API-KEY": self.deps.serperdev_api_key,
                        "Content-Type": "application/json",
                    },
                    json=search_params,
                    timeout=self.timeout,
                )

            logger.info("SerperDev response status", status=response.status_code)
            if response.status_code == 200:
                data = response.json()
                organic = data.get("organic", [])
                if organic:
                    logger.info("‚úÖ SerperDev success", count=len(organic))
                    return [
                        SearchSource(
                            title=r.get("title", ""),
                            url=r.get("link", ""),
                            snippet=r.get("snippet", ""),
                            relevance=0.8,
                            source_type="web",
                        )
                        for r in organic
                    ]
                else:
                    logger.warning("SerperDev returned 0 results")
            else:
                logger.warning("SerperDev non-200", status=response.status_code)
        except Exception as e:
            logger.error("SerperDev error", error=str(e))

        return []

    async def enrich_sources_with_content(
        self, sources: list[SearchSource]
    ) -> list[SearchSource]:
        """Enrich search sources by crawling URLs for full content.

        Args:
            sources: Search sources with snippets

        Returns:
            Same sources enriched with full content from crawling

        Example:
            >>> enriched = await agent.enrich_sources_with_content(sources)
            >>> assert enriched[0].content is not None
        """
        if not self.deps.enable_crawling:
            logger.info("‚è≠Ô∏è  URL crawling disabled, skipping content enrichment")
            return sources

        # Select top URLs to crawl (by relevance)
        urls_to_crawl = sources[: self.deps.max_crawl_urls]
        logger.info(
            f"üåê Crawling {len(urls_to_crawl)} URLs for full content extraction"
        )

        # Crawl URLs in parallel
        crawl_tasks = []
        for source in urls_to_crawl:
            crawl_tasks.append(self._crawl_single_url(source))

        results = await asyncio.gather(*crawl_tasks, return_exceptions=True)

        # Count successful crawls
        success_count = sum(
            1
            for r, source in zip(results, urls_to_crawl)
            if not isinstance(r, Exception) and source.content
        )
        logger.info(f"‚úÖ Successfully crawled {success_count}/{len(urls_to_crawl)} URLs")

        return sources

    def _is_document_url(self, url: str) -> bool:
        """Check if URL points to a document (PDF, DOCX, etc.).

        Args:
            url: URL to check

        Returns:
            True if URL is a document, False otherwise
        """
        url_lower = url.lower()
        doc_extensions = ['.pdf', '.docx', '.doc', '.xlsx', '.xls', '.pptx', '.ppt']
        
        # Check file extension in path
        for ext in doc_extensions:
            if ext in url_lower:
                return True
        
        # Check for common PDF URL patterns
        if 'pdf' in url_lower or 'download' in url_lower or 'arxiv.org/pdf' in url_lower:
            return True
            
        return False
    
    def _get_article_css_selector(self, url: str) -> str | None:
        """Get CSS selector for main article content based on URL domain.
        
        Args:
            url: URL being crawled
            
        Returns:
            CSS selector string or None for default extraction
        """
        # Common article content selectors by domain
        domain_selectors = {
            'bbc.com': 'article, [data-component="text-block"]',
            'cnn.com': 'article, .article__content',
            'nytimes.com': 'article, .StoryBodyCompanionColumn',
            'reuters.com': 'article, [data-testid="ArticleBody"]',
            'theguardian.com': 'article, .article-body-commercial-selector',
            'forbes.com': 'article, .article-body',
            'techcrunch.com': 'article, .article-content',
            'wired.com': 'article, .body__inner-container',
            'ndtv.com': 'article, .sp-cn, #ins_storybody',
            'thehindu.com': 'article, .articlebodycontent',
            'indianexpress.com': 'article, .story_details',
            'hindustantimes.com': 'article, .detail',
            'timesofindia.com': 'article, .article_content',
            'theprin.com': 'article, .entry-content',
            'cnbctv18.com': 'article, .article-content',
            'foxnews.com': 'article, .article-body',
            'apnews.com': 'article, [data-key="article"]',
        }
        
        # Generic fallback for common article tags
        generic_selector = 'article, main, [role="main"], .article, .post-content, .entry-content'
        
        # Extract domain from URL
        from urllib.parse import urlparse
        domain = urlparse(url).netloc.lower()
        domain = domain.replace('www.', '')
        
        # Check for specific domain matches
        for key, selector in domain_selectors.items():
            if key in domain:
                logger.info(f"Using domain-specific selector for {domain}: {selector}")
                return selector
        
        # Use generic selector for unknown domains
        logger.info(f"Using generic article selector for {domain}")
        return generic_selector

    async def _crawl_single_url(self, source: SearchSource) -> SearchSource:
        """Crawl a single URL and update source with content.
        
        Routes to appropriate processor:
        - PDF/DOCX/etc ‚Üí DocklingProcessor for structured extraction
        - Regular web pages ‚Üí Crawl4AI for HTML crawling

        Args:
            source: Search source to enrich

        Returns:
            Same source with content field populated
        """
        try:
            # Check if this is a document URL
            if self._is_document_url(source.url):
                logger.info(f"ÔøΩ Processing document: {source.url}")
                
                # Download document content
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.get(source.url, follow_redirects=True)
                    response.raise_for_status()
                    
                    # Extract filename from URL or Content-Disposition
                    filename = source.url.split('/')[-1]
                    if '?' in filename:
                        filename = filename.split('?')[0]
                    if not any(ext in filename.lower() for ext in ['.pdf', '.docx', '.doc', '.xlsx', '.xls', '.pptx', '.ppt']):
                        filename += '.pdf'  # Default to PDF if no extension
                    
                    # Process document with Dockling
                    processed_doc = await self.deps.document_processor.process_document_bytes(
                        content=response.content,
                        filename=filename,
                        source_url=source.url
                    )
                    
                    # Use processed markdown content
                    source.content = processed_doc.content[:10000]  # Limit to 10K chars
                    logger.info(
                        f"‚úÖ Processed document {source.url}: {len(source.content)} chars, "
                        f"{len(processed_doc.chunks)} chunks"
                    )
                    
            else:
                # Regular web page - use Crawl4AI
                logger.info(f"ÔøΩüï∑Ô∏è  Crawling web page: {source.url}")
                crawled_page = await self.deps.crawl_client.crawl_url(
                    url=source.url,
                    word_count_threshold=50,  # Filter out short/nav blocks
                )

                if crawled_page.success and crawled_page.markdown:
                    # Use markdown content (cleaner than HTML)
                    source.content = crawled_page.markdown[:10000]  # Limit to 10K chars
                    logger.info(
                        f"‚úÖ Crawled {source.url}: {len(source.content)} chars"
                    )
                else:
                    logger.warning(
                        f"‚ö†Ô∏è  Crawl failed for {source.url}: {crawled_page.error_message}"
                    )

        except Exception as e:
            logger.error(f"‚ùå Error processing {source.url}: {e}")

        return source

    async def rank_results(
        self, sources: list[SearchSource], original_query: str
    ) -> list[SearchSource]:
        """Rank and filter results by relevance with optional semantic reranking.

        Args:
            sources: Raw search results
            original_query: User's original question

        Returns:
            Top-ranked sources (up to max_sources), filtered by quality

        Example:
            >>> ranked = await agent.rank_results(sources, "AI agents")
            >>> assert ranked[0].final_score >= ranked[-1].final_score
        """
        if not sources:
            return []

        # Filter low-quality results (relevance < 0.5)
        filtered = [s for s in sources if s.relevance >= 0.5]

        if not filtered:
            return []

        # Apply semantic reranking if enabled
        if self.deps.enable_reranking:
            try:
                logger.info(
                    "Applying semantic reranking",
                    query=original_query,
                    num_candidates=len(filtered),
                    diversity_enabled=self.deps.enable_diversity_penalty,
                    recency_enabled=self.deps.enable_recency_boost,
                    query_aware_enabled=self.deps.enable_query_aware,
                )

                # Prepare documents for reranking (use content if available, else snippet)
                documents = [
                    (s.content if s.content else s.snippet)
                    for s in filtered
                ]

                # Check if enhanced reranking is enabled
                use_enhanced = (
                    self.deps.enable_diversity_penalty
                    or self.deps.enable_recency_boost
                    or self.deps.enable_query_aware
                )

                if use_enhanced:
                    # Use enhanced semantic reranking
                    # Build reranking config
                    config = self.deps.reranking_config or RerankingConfig(
                        diversity_penalty=0.3 if self.deps.enable_diversity_penalty else 0.0,
                        query_aware=self.deps.enable_query_aware,
                        recency_config=RecencyConfig(
                            enabled=self.deps.enable_recency_boost,
                            weight=0.3,
                            adaptive=True,
                        ),
                    )
                    
                    reranker = SemanticReranker(
                        embedding_service=self.deps.embedding_service,
                        config=config,
                    )
                    
                    # Apply enhanced reranking based on enabled features
                    if self.deps.enable_diversity_penalty and not self.deps.enable_recency_boost:
                        rerank_results = await reranker.rerank_with_diversity(
                            query=original_query,
                            documents=documents,
                        )
                    elif self.deps.enable_recency_boost and not self.deps.enable_diversity_penalty:
                        # Need DocumentWithMetadata for recency
                        from src.services.embedding.semantic_reranker import DocumentWithMetadata
                        docs_with_meta = [
                            DocumentWithMetadata(
                                content=doc,
                                published_at=filtered[i].published_at if hasattr(filtered[i], 'published_at') else None,
                            )
                            for i, doc in enumerate(documents)
                        ]
                        rerank_results = await reranker.rerank_with_recency(
                            query=original_query,
                            documents=docs_with_meta,
                        )
                    elif self.deps.enable_query_aware:
                        rerank_results = await reranker.rerank_with_query_awareness(
                            query=original_query,
                            documents=documents,
                        )
                    else:
                        # Multiple features enabled - use query awareness as primary
                        rerank_results = await reranker.rerank_with_query_awareness(
                            query=original_query,
                            documents=documents,
                        )
                    
                    logger.info(
                        "Enhanced semantic reranking applied",
                        method="diversity" if self.deps.enable_diversity_penalty else (
                            "recency" if self.deps.enable_recency_boost else "query_aware"
                        ),
                    )
                else:
                    # Use standard cross-encoder reranking
                    rerank_results = await self.deps.embedding_service.rerank(
                        query=original_query,
                        documents=documents,
                    )

                # Map semantic scores back to sources
                for idx, score in rerank_results:
                    filtered[idx].semantic_score = score

                    # Compute final score as weighted combination
                    # final_score = (1 - w) * relevance + w * semantic_score
                    relevance_weight = 1.0 - self.deps.rerank_weight
                    filtered[idx].final_score = (
                        relevance_weight * filtered[idx].relevance
                        + self.deps.rerank_weight * score
                    )

                logger.info(
                    "Semantic reranking complete",
                    reranked_count=len(rerank_results),
                )

            except Exception as e:
                logger.warning(
                    "Semantic reranking failed, falling back to relevance",
                    error=str(e),
                )
                # Fallback: use relevance as final_score
                for source in filtered:
                    source.final_score = source.relevance
        else:
            # No reranking: use relevance as final_score
            for source in filtered:
                source.final_score = source.relevance

        # Sort by final_score (descending)
        filtered.sort(key=lambda s: s.final_score, reverse=True)

        # Return top max_sources
        return filtered[: self.max_sources]



    async def generate_answer(
        self, query: str, sources: list[SearchSource]
    ) -> str:
        """Generate AI answer based on search sources.

        Args:
            query: User's original query
            sources: Retrieved and ranked sources

        Returns:
            AI-generated answer synthesizing the sources

        Example:
            >>> answer = await agent.generate_answer("What are AI agents?", sources)
            >>> assert len(answer) > 100
        """
        if not sources:
            return "I couldn't find enough information to answer your question. Please try rephrasing your query."

        # Build context from sources
        context_parts = []
        for idx, source in enumerate(sources[:7], 1):  # Use top 7 sources
            # Use full content if available, otherwise fall back to snippet
            content_text = source.content if source.content else source.snippet
            # Limit content length per source (max 8000 chars to capture full articles)
            content_text = content_text[:8000] if len(content_text) > 8000 else content_text
            
            context_parts.append(
                f"[{idx}] {source.title}\nContent: {content_text}\nURL: {source.url}\n"
            )
        context = "\n".join(context_parts)
        
        # Log content enrichment stats
        enriched_count = sum(1 for s in sources[:7] if s.content)
        logger.info(f"üìä Using {enriched_count}/7 sources with full content, {7-enriched_count} with snippets only")

        # ============ CRITICAL LOG: ANSWER GENERATION INPUT ============
        logger.info(f"üí¨ ANSWER GENERATION START")
        logger.info(f"üì• Query: '{query}'")
        logger.info(f"üì• Sources: {len(sources)} total, using top 7")
        logger.info(f"üì• Context length: {len(context)} chars")
        
        # Build prompt for answer generation
        prompt = f"""You are a helpful search assistant. Answer the user's question by extracting and explaining SPECIFIC information from the search results.

User Question: {query}

Search Results:
{context}

MANDATORY REQUIREMENTS:

1. EXTRACT SPECIFIC DETAILS - For EVERY point you make, include:
   ‚úì Exact names, products, services, features mentioned in sources
   ‚úì Specific numbers, dates, percentages, metrics
   ‚úì Direct facts and statements from the sources
   ‚úó NO generic statements like "continues to evolve" or "recent updates"
   ‚úó NO meta-commentary about "the search results show..."
   
   ‚ö†Ô∏è CRITICAL: DO NOT DESCRIBE THE WEBSITES OR WHAT THEY COVER!
   
   ABSOLUTELY FORBIDDEN - DO NOT WRITE THESE PATTERNS:
   ‚úó "ThePrint provides coverage of..." 
   ‚úó "CNBC TV18 reports on business news..."
   ‚úó "Fox News includes a category for..."
   ‚úó "Reuters offers news about..."
   ‚úó "Website X covers topics like A, B, C..."
   ‚úó "Platform Y is providing insightful analyses..."
   ‚úó "Source Z functions as digital platform for..."
   
   IF THE SOURCE ONLY DESCRIBES WEBSITE FEATURES/CATEGORIES:
   ‚Üí SKIP IT ENTIRELY! Say "No specific news events found in sources"
   
   ONLY WRITE ABOUT ACTUAL EVENTS/FACTS:
   ‚úì RIGHT: "India's GDP grew 7.8% in Q2 2024 [1]"
   ‚úì RIGHT: "Microsoft announced Azure AI updates on Nov 10, 2024 [2]"
   ‚úì RIGHT: "Prime Minister Modi met with US President on Nov 8, 2024 [3]"
   ‚úì RIGHT: "Indian rupee weakened to 83.2 against dollar on Nov 11 [4]"

2. EXPLAIN EVERY ITEM - When listing products, services, or features:
   ‚úó BAD: "Azure AI Foundry, Azure AI Search, Azure OpenAI [7]"
   ‚úì GOOD: 
     "- Azure AI Foundry: Platform for building AI applications [7]
      - Azure AI Search: Vector search and retrieval service [7]
      - Azure OpenAI: Access to GPT-4 and other models [7]"
   
   RULE: Never just list names - always add what they do/are

3. MORE EXAMPLES:
   ‚úó BAD: "Growth of 31% [1], stock up 17.9% [1]"
   ‚úì GOOD: "Azure cloud revenue grew 31% year-over-year [1], contributing to Microsoft's stock price increase of 17.9% year-to-date [1], driven by enterprise cloud adoption"
   
   ‚úó BAD: "Microsoft Ignite 2025 on November 17-21 [2]"
   ‚úì GOOD: "Microsoft Ignite 2025 will take place November 17-21, 2025 [2], featuring keynotes on Azure AI capabilities, hands-on labs for developers, and announcements of new cloud services [2]"

4. STRUCTURE with numbered sections and bullet points with descriptions

5. CITE EVERYTHING with [number] after each fact

6. 500+ words - extract and EXPLAIN more details, don't just list

7. NO apologizing or hedging

8. Add context from sources for every claim - "what", "why", "how", "when"

Write a detailed answer with full explanations for everything:"""

        # ============ CRITICAL LOG: LLM CALL FOR ANSWER ============
        logger.info(f"ü§ñ LLM CALL: Answer Generation")
        logger.info(f"üì§ Model: google/gemini-2.5-flash-lite (final answer generation)")
        logger.info(f"üì§ Temperature: 0.7, Max Tokens: 2048")
        logger.info(f"üì§ Prompt length: {len(prompt)} chars")
        
        # ============ FULL PROMPT LOGGING ============
        logger.info("=" * 80)
        logger.info("ÔøΩ FULL LLM INPUT (SEARCH - ANSWER GENERATION)")
        logger.info("=" * 80)
        logger.info(prompt)
        logger.info("=" * 80)

        try:
            # Use Gemini 2.5 Flash Lite for final answer generation
            original_model = self.deps.llm_client.model
            self.deps.llm_client.model = "google/gemini-2.5-flash-lite"
            
            response = await self.deps.llm_client.chat(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=2048,  # Allow longer, more detailed responses
            )
            
            # Restore original model
            self.deps.llm_client.model = original_model

            # ============ CRITICAL LOG: LLM RESPONSE ============
            logger.info(f"üì• LLM RESPONSE received")
            
            # Extract answer from response
            if isinstance(response, dict) and "content" in response:
                answer = response["content"].strip()
                word_count = len(answer.split())
                logger.info(f"üì• Answer length: {len(answer)} chars, {word_count} words")
                
                # ============ FULL RESPONSE LOGGING ============
                logger.info("=" * 80)
                logger.info("üì• FULL LLM OUTPUT (SEARCH - ANSWER)")
                logger.info("=" * 80)
                logger.info(answer)
                logger.info("=" * 80)
                
                logger.info(f"‚úÖ ANSWER GENERATION COMPLETE")
                return answer
            else:
                logger.warning("‚ö†Ô∏è LLM response format unexpected, using fallback")
                logger.warning(f"Response type: {type(response)}, keys: {response.keys() if isinstance(response, dict) else 'N/A'}")
                return self._generate_fallback_answer(query, sources)

        except Exception as e:
            logger.error(f"‚ùå Error generating answer: {e}")
            return self._generate_fallback_answer(query, sources)

    def _generate_fallback_answer(
        self, query: str, sources: list[SearchSource]
    ) -> str:
        """Generate a simple fallback answer if LLM fails.

        Args:
            query: User's query
            sources: Retrieved sources

        Returns:
            Simple concatenated answer from snippets
        """
        if not sources:
            return "No information found."

        # Combine top 3 snippets
        snippets = [s.snippet for s in sources[:3]]
        answer = f"Based on the search results:\n\n{' '.join(snippets)}"
        return answer

    async def validate_output(self, output: SearchOutput) -> SearchOutput:
        """Validate search output meets quality criteria.

        Args:
            output: SearchOutput to validate

        Returns:
            Validated SearchOutput

        Raises:
            ModelRetry: If output doesn't meet quality criteria
        """
        # Validate minimum sources requirement
        if len(output.sources) < self.deps.min_sources:
            raise ModelRetry(
                f"Need at least {self.deps.min_sources} sources, got {len(output.sources)}"
            )

        # Validate minimum confidence threshold
        if output.confidence < self.deps.min_confidence:
            raise ModelRetry(
                f"Confidence {output.confidence:.2f} below minimum {self.deps.min_confidence:.2f}"
            )

        return output

    async def run(
        self,
        query: str,
        mode: SearchMode | str | None = None,
    ) -> SearchOutput:
        """Execute full search workflow with configurable mode.

        Args:
            query: User's search query
            mode: Search mode (SPEED/BALANCED/DEEP) or mode string or None (defaults to BALANCED)

        Returns:
            SearchOutput with results, metadata

        Example:
            >>> result = await agent.run("What are AI agents?", mode=SearchMode.DEEP)
            >>> print(f"Confidence: {result.confidence:.2f}")
            >>> print(f"Sources: {len(result.sources)}")
        """
        import time

        # Parse mode
        if isinstance(mode, str):
            search_mode = get_mode_from_string(mode)
        elif mode is None:
            search_mode = SearchMode.BALANCED
        else:
            search_mode = mode
        
        config = search_mode.config
        
        logger.info(
            "Starting search with mode",
            query=query,
            mode=search_mode.value,
            max_sources=config.max_sources,
            timeout=config.timeout,
        )

        start_time = time.time()

        # Apply mode configuration
        original_max_sources = self.max_sources
        original_enable_reranking = self.deps.enable_reranking
        original_max_crawl_urls = self.deps.max_crawl_urls
        
        self.max_sources = config.max_sources
        self.deps.enable_reranking = config.enable_reranking
        
        # Adjust max_crawl_urls based on mode for better performance
        # BALANCED: crawl top 5 (half of sources) for speed
        # DEEP: crawl all sources for comprehensiveness
        if search_mode == SearchMode.BALANCED:
            self.deps.max_crawl_urls = min(5, config.max_sources // 2)
        elif search_mode == SearchMode.DEEP:
            self.deps.max_crawl_urls = config.max_sources
        # SPEED doesn't crawl, so no need to set

        try:
            # 1. Decompose query
            sub_queries = await self.decompose_query(query)

            # 2. Coordinate search
            raw_sources = await self.coordinate_search(sub_queries)

            # 3. Enrich sources with full content (crawl URLs) - only if enabled by mode
            if config.enable_crawling:
                enriched_sources = await self.enrich_sources_with_content(raw_sources)
            else:
                # SPEED mode: skip crawling, use snippets only
                logger.info("Crawling disabled by mode, using snippets only")
                enriched_sources = raw_sources

            # 3.5. Temporal validation (post-retrieval filtering - Big Tech approach)
            logger.info(f"üïí TEMPORAL VALIDATION START")
            logger.info(f"üìä Input: {len(enriched_sources)} sources before validation")
            
            # Extract temporal intent from sub_queries
            target_year = None
            temporal_scope = "any"
            
            for sq in sub_queries:
                if sq.specific_year:
                    target_year = sq.specific_year
                    logger.info(f"  Detected target year: {target_year}")
                    break
                elif sq.temporal_scope != "any":
                    temporal_scope = sq.temporal_scope
                    logger.info(f"  Detected temporal scope: {temporal_scope}")
            
            # Apply temporal validation (penalize mismatched sources)
            validated_sources = self.temporal_validator.filter_and_rerank_sources(
                sources=enriched_sources,
                target_year=target_year,
                temporal_scope=temporal_scope,
                strict_filtering=False  # Penalize, don't remove
            )
            
            logger.info(f"‚úÖ TEMPORAL VALIDATION COMPLETE")
            logger.info(f"üìä Output: {len(validated_sources)} sources after validation")

            # 4. Rank results (reranking controlled by mode config)
            ranked_sources = await self.rank_results(validated_sources, query)

            # 5. Generate answer from sources
            answer = await self.generate_answer(query, ranked_sources)

            execution_time = time.time() - start_time

            # 6. Build output
            output = SearchOutput(
                answer=answer,
                sub_queries=sub_queries,
                sources=ranked_sources,
                execution_time=execution_time,
                confidence=0.8,  # Default confidence
            )

            # 7. Validate output
            output = await self.validate_output(output)

            logger.info(
                "Search complete",
                mode=search_mode.value,
                num_sources=len(ranked_sources),
                execution_time=execution_time,
            )

            return output
        
        finally:
            # Restore original settings
            self.max_sources = original_max_sources
            self.deps.enable_reranking = original_enable_reranking
            self.deps.max_crawl_urls = original_max_crawl_urls
