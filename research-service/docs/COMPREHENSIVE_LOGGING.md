# Comprehensive Logging - Model Call Tracking

**Last Updated**: 2025-01-XX  
**Purpose**: Track all LLM calls, rate limits, errors, and phases for debugging

---

## ğŸ¯ Overview

This document describes the comprehensive logging system implemented to track:
- âœ… **Model Usage**: Which model is called at which phase
- âš ï¸ **Rate Limits**: When rate limiting occurs and which model
- âŒ **Errors**: API errors, timeouts, and fallback attempts
- ğŸ”„ **Retries**: Retry attempts with exponential backoff
- ğŸ“Š **Token Usage**: Input/output token counts for each request

---

## ğŸ“ Log Emoji Guide

| Emoji | Meaning | Location |
|-------|---------|----------|
| ğŸ¤– | LLM request start (phase-specific) | search_agent.py |
| âœ… | Successful LLM response | openrouter_client.py |
| âš ï¸ | Rate limit error | openrouter_client.py |
| âŒ | API error or failure | openrouter_client.py |
| ğŸ”„ | Retry attempt or fallback switch | openrouter_client.py |
| â±ï¸ | Timeout error | openrouter_client.py |
| ğŸ“¤ | Request details (model, phase) | search_agent.py |
| ğŸ“¥ | Response details | openrouter_client.py |
| ğŸ“Š | Token usage and metrics | openrouter_client.py |

---

## ğŸ” Logging by Phase

### Phase 1: Query Decomposition

```
ğŸ¤– [PHASE: QUERY DECOMPOSITION] LLM CALL START
ğŸ“¤ Model: google/gemini-2.5-flash-lite
ğŸ“¤ Temperature: 0.3, Max Tokens: 500
ğŸ¤– LLM REQUEST START: model=google/gemini-2.5-flash-lite, temp=0.3, max_tokens=500, messages=X
âœ… LLM RESPONSE SUCCESS: tokens_used=Y (input=A, output=B)
```

**Errors**:
```
âš ï¸ RATE LIMIT ERROR: model=google/gemini-2.5-flash-lite, attempt=1/10
ğŸ”„ RETRY: attempt 2/10, delay=2.0s, model=google/gemini-2.5-flash-lite
```

---

### Phase 2: Answer Generation

```
ğŸ¤– [PHASE: ANSWER GENERATION] LLM CALL START
ğŸ“¤ Model: google/gemini-2.5-flash-lite (final answer generation)
ğŸ“¤ Temperature: 0.7, Max Tokens: 2048
ğŸ¤– LLM REQUEST START: model=google/gemini-2.5-flash-lite, temp=0.7, max_tokens=2048, messages=X
âœ… LLM RESPONSE SUCCESS: tokens_used=Y (input=A, output=B)
```

**Synthesis Model Switch** (if configured):
```
ğŸ”„ [PHASE: ANSWER GENERATION] Switching to synthesis model: google/gemini-2.5-flash-lite -> google/gemini-2.5-flash
```

**Errors**:
```
â±ï¸ TIMEOUT ERROR: model=google/gemini-2.5-flash-lite after 60.0s
ğŸ”„ RETRY: attempt 2/10, delay=4.0s, model=google/gemini-2.5-flash-lite
```

---

### Phase 3: Hallucination Regeneration

```
ğŸ¤– [PHASE: HALLUCINATION REGENERATION] LLM CALL START
ğŸ“¤ Model: google/gemini-2.5-flash-lite
ğŸ“¤ Regenerating due to 25.0% hallucinations
ğŸ¤– LLM REQUEST START: model=google/gemini-2.5-flash-lite, temp=0.3, max_tokens=2048, messages=X
âœ… LLM RESPONSE SUCCESS: tokens_used=Y (input=A, output=B)
```

**Fallback After Rate Limits**:
```
âš ï¸ RATE LIMIT ERROR: model=google/gemini-2.5-flash-lite, attempt=2/10
ğŸ”„ FALLBACK: Switching model after rate limit: google/gemini-2.5-flash-lite -> google/gemini-2.5-flash
ğŸ¤– LLM REQUEST START: model=google/gemini-2.5-flash, temp=0.3, max_tokens=2048, messages=X
âœ… LLM RESPONSE SUCCESS: tokens_used=Y (input=A, output=B)
```

---

## ğŸ› ï¸ Implementation Details

### openrouter_client.py (Lines 72-360)

**Key Features**:
1. **Request Logging** (Line 143):
   - Model name, temperature, max_tokens, message count
   - Timestamp and request start marker

2. **Success Logging** (Lines 180-185):
   - Token usage breakdown (input/output)
   - Response received marker

3. **Rate Limit Handling** (Line 310):
   - Model causing rate limit
   - Attempt number (e.g., "1/10")
   - Retry delay

4. **Fallback Logic** (Lines 320-327):
   - Switches to fallback model after 2 rate limit attempts
   - Logs model switch: old_model -> new_model

5. **Error Logging**:
   - **Timeout** (Line 342): Model, timeout duration
   - **API Error** (Line 356): Model, HTTP status code, error message
   - **Fallback Failure** (Line 325): When fallback model also fails

---

### search_agent.py (Lines 462, 2380, 2529)

**Phase Tags**:
```python
# Query Decomposition
logger.info("ğŸ¤– [PHASE: QUERY DECOMPOSITION] LLM CALL START")

# Answer Generation
logger.info("ğŸ¤– [PHASE: ANSWER GENERATION] LLM CALL START")

# Hallucination Regeneration
logger.info("ğŸ¤– [PHASE: HALLUCINATION REGENERATION] LLM CALL START")
```

**Model Selection Logging**:
```python
# Synthesis model switch (if configured)
if settings.SYNTHESIS_LLM_MODEL:
    logger.info(
        f"ğŸ”„ [PHASE: ANSWER GENERATION] Switching to synthesis model: "
        f"{original_model} -> {settings.SYNTHESIS_LLM_MODEL}"
    )
```

---

## ğŸ”„ Retry Logic

### Exponential Backoff

```
Attempt 1: delay = 1s
Attempt 2: delay = 2s (2^1)
Attempt 3: delay = 4s (2^2)
Attempt 4: delay = 8s (2^3)
...
Attempt 10: delay = 300s (max cap)
```

### Fallback After Rate Limits

```
Rate Limit 1 â†’ Retry with same model (2^1 = 2s delay)
Rate Limit 2 â†’ Switch to fallback model (2^2 = 4s delay)
Rate Limit 3+ â†’ Retry with fallback model
```

---

## ğŸ“Š Example Log Sequence (Rate Limit Scenario)

### Successful Request
```
ğŸ¤– [PHASE: QUERY DECOMPOSITION] LLM CALL START
ğŸ“¤ Model: google/gemini-2.5-flash-lite
ğŸ“¤ Temperature: 0.3, Max Tokens: 500
ğŸ¤– LLM REQUEST START: model=google/gemini-2.5-flash-lite, temp=0.3, max_tokens=500, messages=3
âœ… LLM RESPONSE SUCCESS: tokens_used=150 (input=120, output=30)
```

### Rate Limit â†’ Retry â†’ Fallback
```
ğŸ¤– [PHASE: ANSWER GENERATION] LLM CALL START
ğŸ“¤ Model: google/gemini-2.5-flash-lite (final answer generation)
ğŸ“¤ Temperature: 0.7, Max Tokens: 2048
ğŸ¤– LLM REQUEST START: model=google/gemini-2.5-flash-lite, temp=0.7, max_tokens=2048, messages=5

âš ï¸ RATE LIMIT ERROR: model=google/gemini-2.5-flash-lite, attempt=1/10
ğŸ”„ RETRY: attempt 2/10, delay=2.0s, model=google/gemini-2.5-flash-lite
ğŸ¤– LLM REQUEST START: model=google/gemini-2.5-flash-lite, temp=0.7, max_tokens=2048, messages=5

âš ï¸ RATE LIMIT ERROR: model=google/gemini-2.5-flash-lite, attempt=2/10
ğŸ”„ FALLBACK: Switching model after rate limit: google/gemini-2.5-flash-lite -> google/gemini-2.5-flash
ğŸ”„ RETRY: attempt 3/10, delay=4.0s, model=google/gemini-2.5-flash
ğŸ¤– LLM REQUEST START: model=google/gemini-2.5-flash, temp=0.7, max_tokens=2048, messages=5

âœ… LLM RESPONSE SUCCESS: tokens_used=800 (input=600, output=200)
```

---

## ğŸ§ª Testing the Logging

### Test Command
```bash
# Run a search query to see all logs
curl -X POST http://localhost:8000/v1/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "GitHub Universe 2024",
    "search_mode": "semantic",
    "max_sources": 10,
    "enable_hallucination_detection": true
  }'
```

### View Logs
```bash
# Follow container logs
docker-compose logs -f research-api

# Filter for specific log types
docker-compose logs research-api | grep "ğŸ¤–"  # LLM requests
docker-compose logs research-api | grep "âš ï¸"  # Rate limits
docker-compose logs research-api | grep "âŒ"  # Errors
docker-compose logs research-api | grep "ğŸ”„"  # Retries/fallbacks
```

---

## ğŸ“‹ Configuration

### Environment Variables

```bash
# Model Configuration
LLM_MODEL=google/gemini-2.5-flash-lite
RESEARCH_LLM_MODEL=google/gemini-2.5-flash-lite
FALLBACK_LLM_MODEL=google/gemini-2.5-flash
SYNTHESIS_LLM_MODEL=  # Optional: dedicated synthesis model

# Retry Configuration (defaults)
MAX_RETRIES=10
INITIAL_DELAY=1.0
MAX_DELAY=300.0
RATE_LIMIT_ATTEMPTS_BEFORE_FALLBACK=2
```

---

## ğŸ”§ Troubleshooting

### No Logs Appearing
1. Check container is running: `docker-compose ps`
2. Verify log level: `LOG_LEVEL=INFO` in .env
3. Rebuild container: `docker-compose build research-api`
4. Restart container: `docker-compose restart research-api`

### Rate Limits Not Triggering Fallback
1. Check `FALLBACK_LLM_MODEL` is set in .env
2. Verify `RATE_LIMIT_ATTEMPTS_BEFORE_FALLBACK=2` (default)
3. Look for "ğŸ”„ FALLBACK: Switching model" in logs

### Missing Phase Tags
1. Ensure search_agent.py has been updated with PHASE tags
2. Check lines 462, 2380, 2529 in search_agent.py
3. Rebuild container after code changes

---

## ğŸ“š Related Documentation

- **Process Flows**: `docs/PROCESS_FLOWS.md`
- **Development Standards**: `docs/DEVELOPMENT_STANDARDS.md`
- **Two-Pass Synthesis**: `docs/TWO_PASS_SYNTHESIS_IMPLEMENTATION.md`
- **Query Expansion**: `docs/QUERY_EXPANSION_SUMMARY.md`

---

**REMEMBER**: All logs use emojis for quick visual scanning. Look for ğŸ¤–, âš ï¸, âŒ, and ğŸ”„ markers! ğŸ¯
